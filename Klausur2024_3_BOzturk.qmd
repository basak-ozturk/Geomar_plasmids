---
title: "Exam Statistics 2024_3"
author: Your Name
format: docx
editor: visual
markdown: 
  wrap: 60
---

```{r}
#any packages needed? put them here!
pacman::p_load(conflicted,
               tidyverse,
               wrappedtools,
               ggbeeswarm,
               flextable,
               readxl
)



conflicts_prefer(dplyr::filter)
```

Please give your answers underneath the question, referencing question numbers.

Please format text answers as comments or put them in text parts.

# Q1: Please generate the following sequences of numbers:

## Q1a: Integers between 95 and 45 in descending order, including borders

```{r}
set.seed(123)
Q1a<-seq(from=95, to=45, by=-1)
```

## Q1b: 50 random numbers from a uniform distribution with minimum=18 and maximum=85

```{r}
set.seed(456)
Q1b<-runif(50, min=18, max=85)
```

## Q1c: 250 random numbers from a Normal distribution with mean=10 and SD=2

```{r}
set.seed(798)
Q1c<-rnorm(250, mean=10, sd=2)
```

## Q1d: 20 random numbers from a Poisson distribution with lambda=5

```{r}
set.seed(101112)
Q1d<-rpois(20, lambda = 5)
```

## Q1e: 6 Lotto numbers (unique, uniform distribution) between 1 and 49 ("6 aus 49").

```{r}
set.seed(131415)
Q1e<- sample(1:49, size=6, replace= FALSE)
Q1e
```

## Q2: Test the various numbers you generated in Q1 against the Normal distribution, create a table structure with the p-values and your interpretation

```{r}

#data exploration for Q1c
Q1c_data<-data.frame(values = Q1c)
ggplot(Q1c_data, aes(x = values)) +
  geom_density(fill = "pink", alpha = 0.5) +
  theme_minimal()

# Shapiro-Wilk test
shapiro_p_value_Q1c <- shapiro.test(Q1c)$p.value

#data exploration for Q1a
Q1a_data<-data.frame(values = Q1a)
ggplot(Q1a_data, aes(x = values)) +
  geom_density(fill = "lightblue", alpha = 0.5) +
  theme_minimal()

# Shapiro-Wilk test
shapiro_p_value_Q1a <- shapiro.test(Q1a)$p.value

#data exploration for Q1b
Q1b_data<-data.frame(values = Q1b)
ggplot(Q1b_data, aes(x = values)) +
  geom_density(fill = "lightgreen", alpha = 0.5) +
  theme_minimal()

# Shapiro-Wilk test
shapiro_p_value_Q1b <- shapiro.test(Q1b)$p.value

#I have chosen to stick with these three as the sample sizes for the others are too small

shapiro_results <- tibble(
  numbers = c("Q1a", "Q1b", "Q1c"),
  p_value = c(
    shapiro_p_value_Q1a,
    shapiro_p_value_Q1b,
    shapiro_p_value_Q1c
  )
 
)
results_table <- flextable(shapiro_results) 

#I chose the Shapiro test due to the small sample size in each number. A large p-Value (p>0.05) would indicate normal distribution. Q1c is normally distributed, as expected. Q1b is not normally distributed, as also evident from the density plot. Q1a is on the border, the p-value is slightly larger than 0.05 but the density plot suggests that it is not normally distributed, so I would say it doesn't show normal distribution.
```

# Q3: Please explain the following terms in your own words (rather than just copy/pasting from wikipedia/chatbots)

## Q3a: standard deviation

Standard deviation is a measure of how dispersed the data is. If the standard deviation is low, the data points are all close to the mean value. If it is high, the difference between the mean and the individual data points is high, indicating that the data is highly dispersed. It is not affected by the sample size.

## Q3b: standard error of the mean

The standard error of the mean indicates how precisely we were able to calculate the mean value for a given dataset. This is measured by dividing the standard deviation with the square root of the sample size, so it is dependent on the sample size. Reducing the SEM by ten for example would take hundred times more observations.

## Q3c: significance (as in 'we found a *significant* difference)

Statistical significance means that if our null hypothesis is true, the observation that we made is very unlikely to be because of coincidence or random chance. For example if our null-hypothesis is that a treatment has no effect, for this hypothesis to be rejected the observation has to be statistically significant (lower than the significance level that was previously specified). The p-value is the probability of observing the results that we have obtained if the null hypothesis was true.

## Q3d: statistical power

Statistical power shows how effective an experiment or study was in measuring a real effect. If our null hypothesis is that a treatment has no effect and the alternative hypothesis is that the treatment has an effect, there can be two types of errors. We can reject the null hypothesis although there is indeed no effect and the null hypothesis is true (Type I) or we can fail to reject the null hypothesis when it is indeed false (Type II). In this case we would conclude that the treatment has no effect, when it actually does. Statistical power is the probability of avoiding the Type II error. For example if our statistical power is 80%, only 80 out of 100 statistical tests will detect a real effect when 100 real effects are present.

## Q3e: sensitivity

Sensitivity is the likelihood of correctly detecting a positive case. For example, in case of the Plamer penguins, if a regression tree can correctly classify 90 Adelie penguins out of 100, its sensitivity is 90%.

## Q3f: specificity

Specificity is the likelihood to correctly detecting a negative case and avoiding false negatives. For example, if our model can correctly identify 90 out of 100 non-Adelie penguins as not being Adelie, its specificity is 90%.

# Q4: Please name the class / function for test statistics in the following cases:

## Q4a: Comparison of mean values for 2 groups for a Gaussian measure

T-test could be used:

t.test(x,y)

## Q4b: Comparison of central tendencies for ordinal data between two groups; what descriptive statistic would be suitable together with the test?

For ordinal data, Wilcoxon rank sum test can be used:

wilcox.test(x,y)

In addition, the median (central tendency) and interquartile range (spread of the data) can be determined for ordinal data.

# Q5: Please import and analyze the dataset bordeaux.xlsx

Info on dataset:

-   year: year of harvest

-   temperature: sum of daily average temperatures (in Celsius degrees)

-   sun: duration of insolation (in hours)

-   heat: number of super-hot days

-   rain: rain level (in millimeters)

-   quality: wine quality: a factor with levels bad, good, and medium

## Q5a: Program the import and "pick" / address the first 5 rows from columns rain and quality

```{r}

datab <- read_excel(
  "../R_files/data/bordeaux.xlsx",
  col_names = T
)

rain_quality <- tibble(datab[1:5,5],
                       datab[1:5,6]
)


```

## Q5b: Create a box plot for rain level, grouped by quality, using ggplot. Try to use a reasonable order of quality levels

```{r}

rain_quality$quality<-as.factor(rain_quality$quality) #convert quality to factor for sorting

ggplot(rain_quality, aes(x = quality, y = rain)) +
  geom_boxplot()

```

## Q5c: Test, if the group differences you observe in Q5b could be a chance outcome, formulate the appropriate Null hypothesis. (keep in mind that '*dependent variable*' does not imply causality)

```{r}
# Null hypothesis: There is no difference in rain amount between the different quality groups 

lm_rq <- lm(rain~quality, data = rain_quality) #linear model

anova_rq<- anova(lm_rq) #type I anova

summary(anova_rq)
```

## Q5d: Create a report table with descriptive statistics for temperature and rain for the levels of quality.

```{r}

datab$quality<-as.factor(datab$quality)

ggplot(datab, aes(x = temperature)) +
  geom_density(fill = "pink", alpha = 0.5) 

shapiro.test(datab$temperature)$p.value #p value = 0.23, normally distributed

ggplot(datab, aes(x = rain)) +
  geom_density(fill = "lightblue", alpha = 0.5) 

shapiro.test(datab$rain)$p.value # p value = 0.007, not normally distributed

#scatter plot with color by quality
ggplot(datab, aes(x = temperature, y = rain, color = quality)) +
  geom_point() +
  theme_minimal()



stats_for_table <- datab |>
  group_by(quality) |>
summarise(
    #for normally distributed temperature I chose to report mean and sd
    across(
      temperature,
      list(
        mean = ~mean(.),
        sd = ~sd(.)
      ),
      .names = "temperature_{.fn}"
    ),
    #for non-normally distributed "rain" I chose to report median and IQR
    across(
      rain,
      list(
        median = ~median(.),
        iqr = ~IQR(.)
      ),
      .names = "rain_{.fn}"
    )
  )

stats_table <- flextable(stats_for_table) #(didn't manage to export flextable)
stats_table2 <- table(stats_for_table)

stats_table
stats_table2
```

## Q5e: Export that table into a text file with a ';' as separator

```{r}

write.table(stats_table2, file = "../R_files/data/bordeaux_output.txt", sep = ";", row.names = FALSE, col.names = TRUE)
```

# Q6: Create a matrix named 'participants' with 3 columns and 20 rows.

-   Fill column 1 with the words "participant 1" to "participant 20"

-   Put your first name in row 2 column 2 and your family name in row 2 column 3

```{r}

participants <- matrix("na",nrow = 20, ncol = 3)

for (n in 1:nrow(participants)){
  participants[n,1] = paste0("participant ",n)
}

participants
participants[2,2] = "Basak"
participants[2,3] = "Öztürk"

participants
```

# Q7: Program a loop that runs along the rows of matrix from Q6 and that prints the run number and the first name

```{r}

for (row_n in 1:nrow(participants)) {
  print(paste("Run", row_n, ":", participants[row_n, 2]))
}

#I hope I got it right now

```

# 

***Good luck!***

# 
